# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NBNApBDqT5qZavV7xxPBxqv57K3k4kH7
"""

import pandas as pd

data=pd.read_csv('/content/twitter_training.csv')

data.columns = ['id', 'topic', 'sentiment', 'text']

data

data.info()

data.isnull().sum()

data = data.dropna(subset=['text'])

import re

def clean_text(text):
  text = re.sub(r'http\S+', '', text) # remove urls
  text = re.sub(r'[^a-zA-Z\s]', '', text) # remove non-alphanumeric characters
  text = text.lower()
  return text

data['text'] = data['text'].apply(clean_text)

def tokenize(text):
  return text.split()

data['tokens'] = data['text'].apply(tokenize)

data

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def remove_stopwords(tokens):
  return [word for word in tokens if word not in stop_words]

data['tokens'] = data['tokens'].apply(remove_stopwords)

data

sentiment_mapping = {'Positive': 1, 'Negative': -1, 'Neutral': 0}
data['sentiment_encoded'] = data['sentiment'].map(sentiment_mapping)

data

from sklearn.model_selection import train_test_split

X = data['tokens']
y = data['sentiment_encoded']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check for missing values in 'sentiment_encoded' column
print(data['sentiment_encoded'].isnull().sum())

# Handle missing values in 'sentiment_encoded' column
# For example, you can drop rows with missing values:
data = data.dropna(subset=['sentiment_encoded'])

# Redo the train-test split after handling missing values
X = data['tokens']
y = data['sentiment_encoded']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Continue with vectorization and model training

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train.apply(lambda x: ' '.join(x)))
X_test_vec = vectorizer.transform(X_test.apply(lambda x: ' '.join(x)))

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train_vec, y_train)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Step 1: Encoding Sentiment Labels
label_encoder = LabelEncoder()
data['sentiment_encoded'] = label_encoder.fit_transform(data['sentiment'])

# Step 2: Splitting the Data
X_train, X_test, y_train, y_test = train_test_split(
    data['text'],
    data['sentiment_encoded'],
    test_size=0.2,
    random_state=42
)

# Step 3: Vectorizing the Text Data
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Step 4: Training a Model
model = LogisticRegression(max_iter=1000)
model.fit(X_train_tfidf, y_train)

# Step 5: Evaluating the Model
y_pred = model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)

# Print the results
print(f'Accuracy: {accuracy:.4f}')
print('Classification Report:')
print(report)

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

# Binarize the output labels for multi-class ROC AUC
n_classes = len(label_encoder.classes_)
y_test_bin = label_binarize(y_test, classes=range(n_classes))

# Train the model with OneVsRestClassifier for multi-class ROC
classifier = OneVsRestClassifier(LogisticRegression(max_iter=1000))
y_score = classifier.fit(X_train_tfidf, y_train).decision_function(X_test_tfidf)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot all ROC curves
plt.figure()
colors = ['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple']
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC curve of class {label_encoder.classes_[i]} (area = {roc_auc[i]:0.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score

# Compute Precision-Recall curve and area for each class
precision = dict()
recall = dict()
average_precision = dict()

for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])
    average_precision[i] = average_precision_score(y_test_bin[:, i], y_score[:, i])

# Plot the Precision-Recall curve for each class
plt.figure()
for i, color in zip(range(n_classes), colors):
    plt.plot(recall[i], precision[i], color=color, lw=2,
             label=f'Precision-Recall curve of class {label_encoder.classes_[i]} (area = {average_precision[i]:0.2f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='lower left')
plt.show()

import seaborn as sns

# Plot actual vs predicted class distributions
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.countplot(x=y_test, palette='viridis')
plt.title('Actual Class Distribution')

plt.subplot(1, 2, 2)
sns.countplot(x=y_pred, palette='viridis')
plt.title('Predicted Class Distribution')

plt.show()

import numpy as np

# Get the coefficients from the model
coefficients = model.coef_

# Plot top features for each class
for i, class_label in enumerate(label_encoder.classes_):
    top10 = np.argsort(coefficients[i])[-10:]
    plt.figure(figsize=(8, 6))
    plt.barh(range(10), coefficients[i][top10], align='center')
    plt.yticks(range(10), [vectorizer.get_feature_names_out()[j] for j in top10])
    plt.xlabel('Coefficient Value')
    plt.title(f'Top 10 Features for Class: {class_label}')
    plt.show()